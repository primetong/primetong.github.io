---
layout:     post
title:      "TensorFlow Notes 4 | 【TensorFlow深度学习框架教程四】"
subtitle:   "深层神经网络"
date:       2018-4-3
author:     "Witt"
header-img: "img/post-bg-tensorflow.jpg"
tags:
    - TensorFlow
    - DNN
---

## 深层神经网络（Deep Neural Network）

### 深度学习（Deep Learning）和深层神经网络（DNN）

现在很火的深度学习的概念，其实在实际中基本上可以认为是深层神经网络的代名词。那么为什么要叫“深层”呢？与上一篇笔记中的神经网络又有什么区别呢？  
上一篇笔记中的神经网络，其实使用的是线性模型：y = Σxiwi + b(偏置)，任意线性模型组合仍为线性→单层←→多层，线性模型仅可解决线性可分问题，那么如何去线性化（全连接非线性问题），这时候我们就要引入这篇笔记的主角——深层神经网络。  
深层神经网络有两个非常重要的特性——①非线性、②多层，下面一一阐述  

##### ①非线性

- 相对于以往的神经网络，由于没有使用激活函数，构造出来的函数往往形如：w1x1+w2x2+…+wnxn + b = 0,很显然这种函数只能模拟线性分割。即只能通过直线来划分，一旦分割面是一个圆形，通过这种方式只能尽可能的得到一个多棱角保卫面，而不能拟合成圆形，存在很大的误差。
- 细想一下，如果我们换一种权重作用方式，比如将w1x1换为x1^w1 或者 w1*e^x1,很显然这种指数函数作用的结果是一种弯曲状态，就能够拟合上面所说的圆形。但是，目前我们采用的方式是直接在输出层外加上一层激活函数（弯曲函数），就能够实现这种方式（不同的函数作用效果也不一样）。通过激活函数（NN去线性化）有3种：
	- ①tf.nn.relu()；
	- ②tf.nn.sigmoid()；
	- ③tf.nn.tanh()；